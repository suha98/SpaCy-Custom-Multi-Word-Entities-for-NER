{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime as dt\n",
    "from datetime import time\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import timedelta\n",
    "import regex as re\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.util import filter_spans\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.pipeline import EntityRuler\n",
    "import re\n",
    "from spacy.tokens import Span\n",
    "from spacy.util import filter_spans\n",
    "from spacy.training.example import Example\n",
    "from spacy.language import Language\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tahia.Tabassum\\AppData\\Roaming\\Python\\Python38\\site-packages\\spacy\\util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.2.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "tokenizer = nlp.tokenizer\n",
    "all_stopwords = nlp.Defaults.stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('merged_cats.csv')\n",
    "equip = pd.read_csv('Equipment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning up the dataframe\n",
    "\n",
    "def clean_notes(data, col_name):\n",
    "    data[col_name] = data[col_name].str.replace('lc', 'lamp column')\n",
    "    data[col_name] = data[col_name].str.replace('l/c', 'lamp column')\n",
    "    data[col_name] = data[col_name].str.replace('sugg', 'suggested')\n",
    "    data[col_name] = data[col_name].str.replace('rreturn', 'return')\n",
    "    data[col_name] = data[col_name].str.replace('o/s', 'outside')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_notes(df, 'job_notes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks if a string matches a regex pattern\n",
    "\n",
    "def regex_checker(text, pattern):\n",
    "    matches = re.finditer(pattern, text)\n",
    "    for match in matches:\n",
    "        print (match)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load json file\n",
    "def load_data(file):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return (data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data to a json file\n",
    "def save_data(file, data):\n",
    "    with open (file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforms the data into a json structure\n",
    "#The counter counts the number of unique entities in the data\n",
    "\n",
    "def structure_data(text, doc_param, counter):\n",
    "    results=[]\n",
    "    entities = []\n",
    "    ignore = ['GPE', 'DATE', 'ADDRESS', 'ORG', 'PERSON', 'FAC', 'TIME', 'NORP', 'MONEY', 'LOC']\n",
    "    for ent in doc_param.ents:\n",
    "        if ent.label_ not in ignore:\n",
    "            entities.append((ent.start_char, ent.end_char, ent.label_))\n",
    "            counter[ent.label_] = counter.get(ent.label_, 0) + 1\n",
    "    \n",
    "    if len(entities) > 0:\n",
    "        results = [text, {\"entities\": entities}]\n",
    "    else:\n",
    "        results = None\n",
    "    \n",
    "    return (results), counter\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.entity_removal(doc)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to remove the QUANTITY entity from the list of default entities\n",
    "\n",
    "@Language.component(\"entity_removal\")\n",
    "def entity_removal(doc):\n",
    "    ents = list(doc.ents)\n",
    "    for ent in ents:\n",
    "        print(ent.label_)\n",
    "        if ent.label_=='QUANTITY':\n",
    "            ents.remove(ent)\n",
    "    ents = tuple(ents)\n",
    "    doc.ents = ents\n",
    "    return(doc)\n",
    "Language.component(\"entity_removal\", func=entity_removal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "\n",
    "def train_spacy(data, iterations):\n",
    "    TRAIN_DATA = data\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe(\"ner\", last=True)\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            \n",
    "            ner.add_label(ent[2])\n",
    "            \n",
    "    \n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        nlp.add_pipe(\"entity_removal\", before=\"ner\")\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(iterations):\n",
    "            print (\"Starting iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                # Update the model\n",
    "                nlp.update([example], losses=losses, drop=0.2, sgd = optimizer)\n",
    "            print (losses)\n",
    "    return (nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a csv of keywords needed to be labelled as the 'NEW EQUIPMENT' entity if found in the data\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before = \"ner\")\n",
    "for i in range(len(equip)):    \n",
    "    ruler.add_patterns([{\"label\": \"NEW EQUIPMENT\", \"pattern\": equip.iloc[i,0]}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#patterns to identify new entities\n",
    "patterns =  [{\"label\": \"ADDRESS\", \"pattern\": [\" ?outside \\d+ ?\", \" ?\\d{1,3} [A-Za-z]+ road\\.?\",\n",
    "                                             \" ?\\d{1,3} [A-Za-z]+ rd\\.?\",\" ?\\d{1,3} [A-Za-z]+ hill\\.?\",\n",
    "                                              \" ?\\d{1,3} [A-Za-z]+ Hill\\.?\", \" ?\\d{1,3} st\\.? [A-Za-z]+\",\n",
    "                                              \" ?\\d{1,3} saint\\.? [A-Za-z]+\", \"\\s\\d{1}[A-Za-z]{2}\\s|\\s\\d{1}[A-Za-z]{2}$|^\\d{1}[A-Za-z]{2}\\s\"]},\n",
    "            {\"label\": \"COLUMN MEASUREMENT\", \"pattern\": [\" ?\\d{1,5}\\.?\\d{0,5}n?m column| ?\\d{1,5}\\.?\\d{0,5}N?M column\",\n",
    "                                                       \" ?\\d{1,3}\\.?\\d{0,5} column ?| ?\\d{1,5}\\.?\\d{0,5} meter ?[A-Za-z]*\"]},\n",
    "            {\"label\": \"PHONE NUMBER\", \"pattern\": [\" ?\\d{3} ?\\d{7,8}\"]}, \n",
    "            \n",
    "            {\"label\": \"TEMPERATURE\", \"pattern\": [\" ?\\d{1,4}k ?| ?\\d{1,4}K ?\"]},\n",
    "            {\"label\": \"POWER\", \"pattern\": [\" ?\\d{1,4}w| ?\\d{1,4}W\"]},\n",
    "            {\"label\": \"LANTERN FEATURE\", \"pattern\": [\" ?\\d{1,3} led lantern ?\"]},\n",
    "            \n",
    "            {\"label\": \"LANTERN FEATURE\", \"pattern\": [\" ?\\d{1,3} led lantern ?\"]},\n",
    "            {\"label\": \"REFERENCE\", \"pattern\": [\"(lamp)? ?column \\d{1,4} ?\"]},\n",
    "            {\"label\": \"JOB NUMBER\", \"pattern\": [\"( ?job( number)?| ?quote( number)?| ?quotation( number)?) ?\\d+ ?\"]},\n",
    "            {\"label\": \"CUSTOM_QUANTITY\", \"pattern\": [\" ?remove (one|two|three|four|five|six|seven|eight|nine) [a-z]+ ?[a-z]* ?\",\n",
    "                                              \" ?replace (one|two|three|four|five|six|seven|eight|nine) [a-z]+\\b ?[a-z]*\\b ?\",\n",
    "                                             \" ?[a-z]* ?[a-z]* damaged x\\d{1,3} ?[a-z]* ?\",\n",
    "                                              \" ?x?\\d{1,4} illuminated (sign)? posts? ?\",\n",
    "                                              \" ?x\\d{1,4} ?[a-z]* ?\",\n",
    "                                              \" ?[a-z]* ?[a-z]* \\d* x\\d{1,3} ?[a-z]* ?\",\n",
    "                                              \" ?supply (one|two|three|four|five|six|seven|eight|nine) [a-z]+ ?[a-z]* ?\",\n",
    "                                              \" ?\\d{1,4} ?[a-z]+ to upgrade$ ?\"                 \n",
    "                                             ]}\n",
    "     ]\n",
    "                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset\n",
    "train_df, test_df = train_test_split(df, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'ADDRESS',\n",
       " 'pattern': [' ?outside \\\\d+ ?',\n",
       "  ' ?\\\\d{1,3} [A-Za-z]+ road\\\\.?',\n",
       "  ' ?\\\\d{1,3} [A-Za-z]+ rd\\\\.?',\n",
       "  ' ?\\\\d{1,3} [A-Za-z]+ hill\\\\.?',\n",
       "  ' ?\\\\d{1,3} [A-Za-z]+ Hill\\\\.?',\n",
       "  ' ?\\\\d{1,3} st\\\\.? [A-Za-z]+',\n",
       "  ' ?\\\\d{1,3} saint\\\\.? [A-Za-z]+',\n",
       "  '\\\\s\\\\d{1}[A-Za-z]{2}\\\\s|\\\\s\\\\d{1}[A-Za-z]{2}$|^\\\\d{1}[A-Za-z]{2}\\\\s']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_text = []\n",
    "train = []\n",
    "counter = {}\n",
    "\n",
    "entity_json=[]\n",
    "#list of new entities\n",
    "#nlp.add_pipe(\"entity_removal\", before=\"ner\")\n",
    "\n",
    "for i in range(len(train_df)):\n",
    "    doc = nlp(train_df.iloc[i, 3])\n",
    "    \n",
    "    ents = list(doc.ents)\n",
    "    for ent in ents:\n",
    "        if ent.label_=='QUANTITY':\n",
    "            ents.remove(ent)\n",
    "    ents = tuple(ents)\n",
    "    doc.ents = ents\n",
    "    \n",
    "    original_ents = list(doc.ents)\n",
    "\n",
    "    new_ents = []\n",
    "\n",
    "    #Identifying new multiple-word entities using regex patterns\n",
    "    for m in range(len(patterns)):\n",
    "        for n in range(len(patterns[m]['pattern'])):\n",
    "            for match in re.finditer(patterns[m]['pattern'][n], doc.text):\n",
    "                start, end = match.span()\n",
    "                span = doc.char_span(start, end, alignment_mode = 'expand')\n",
    "                if span is not None:\n",
    "                    #appending start char, end char, and text of entity\n",
    "                    new_ents.append((span.start, span.end, span.text))\n",
    "                    #print(span.text, patterns[m]['label'])\n",
    "                \n",
    "        for ent in new_ents:\n",
    "            start, end, name = ent\n",
    "            per_ent = Span(doc, start, end, label=patterns[m]['label'])\n",
    "            original_ents.append(per_ent)    \n",
    "   \n",
    "    #prioritizing the matches        \n",
    "    filtered = filter_spans(original_ents)\n",
    "    #print(filtered)\n",
    "    #updating entities\n",
    "    doc.ents = filtered \n",
    "    small_entities =[]\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        entity_text.append((ent.text, ent.label_))\n",
    "    x = [train_df.iloc[i,3], {'entities': entity_text}]\n",
    "    if len(entity_text)>0:\n",
    "        entity_json.append(x)\n",
    "    \n",
    "    results, counter = structure_data(train_df.iloc[i, 3], doc, counter)\n",
    "    if results != None:\n",
    "        train.append(results)\n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299\n"
     ]
    }
   ],
   "source": [
    "print (len(train))\n",
    "save_data(\"training_data.json\", train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"labels_training_data.json\", entity_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
    "\n",
    "    Args:\n",
    "        data (list): The data to be cleaned in spaCy JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: The cleaned data.\n",
    "    \"\"\"\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(\n",
    "                    text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 0\n",
      "{'ner': 1499.8445741933665}\n",
      "Starting iteration 1\n",
      "{'ner': 700.0904885224621}\n",
      "Starting iteration 2\n",
      "{'ner': 490.8532871622607}\n",
      "Starting iteration 3\n",
      "{'ner': 437.68283963226963}\n",
      "Starting iteration 4\n",
      "{'ner': 363.7619856078855}\n",
      "Starting iteration 5\n",
      "{'ner': 306.57576515176913}\n",
      "Starting iteration 6\n",
      "{'ner': 277.88810187166524}\n",
      "Starting iteration 7\n",
      "{'ner': 357.5757717234614}\n",
      "Starting iteration 8\n",
      "{'ner': 237.30689356397855}\n",
      "Starting iteration 9\n",
      "{'ner': 223.35312854362647}\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA = load_data(\"training_data.json\")\n",
    "TRAIN_DATA = trim_entity_spans(TRAIN_DATA)\n",
    "random.shuffle(TRAIN_DATA)\n",
    "nlp = train_spacy(TRAIN_DATA, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating the test set from the trained model\n",
    "small_entities =[]\n",
    "for i in range(len(test_df)):\n",
    "    entities = []\n",
    "    doc = nlp(test_df.iloc[i,3])\n",
    "    for ent in doc.ents:\n",
    "        entities.append((ent.text, ent.label_))\n",
    "    results = [test_df.iloc[i,3], {'entities': entities}]\n",
    "    if len(entities)>0:\n",
    "        small_entities.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"test_results.json\", small_entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
